{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![](https://i.postimg.cc/4yw6mBtC/AKbyq-TKUkics-YGx3xwe3-HA.jpg)\n",
    ">Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you’ll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \"statistically significant\" luck!\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\n",
    "1. [Chapter 2 - A Data Science Framework](#ch2)\n",
    "1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\n",
    "1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\n",
    "1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\n",
    "1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\n",
    "1. [Chapter 7 - Step 5: Model Data](#ch7)\n",
    "1. [Chapter 8 - Evaluate Model Performance](#ch8)\n",
    "1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\n",
    "1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\n",
    "1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\n",
    "1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\n",
    "1. [Change Log](#ch90)\n",
    "1. [Credits](#ch91)\n",
    "\n",
    "**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch1\"></a>\n",
    "# How a Data Scientist Beat the Odds\n",
    "It's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Spaceship Titanic, to walk the reader through, how-to use the data science framework to beat the odds."
   ],
   "metadata": {
    "_cell_guid": "ec31d549-537e-4f35-8594-22ef03079081",
    "_uuid": "ff6c70c884a8a5fb3258e7f3fefc9e55261df1df",
    "papermill": {
     "duration": 0.029716,
     "end_time": "2023-05-17T04:47:14.825563",
     "exception": false,
     "start_time": "2023-05-17T04:47:14.795847",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch2\"></a>\n",
    "# A Data Science Framework\n",
    "1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n",
    "2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n",
    "3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n",
    "4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n",
    "5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n",
    "6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\n",
    "7. **Optimize and Strategize:** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate."
   ],
   "metadata": {
    "_cell_guid": "8514a868-6816-494a-9e9c-dba14cb36f42",
    "_uuid": "bd2a68dcaace0c1371f2994f0204dce65800cc20",
    "papermill": {
     "duration": 0.027682,
     "end_time": "2023-05-17T04:47:14.881557",
     "exception": false,
     "start_time": "2023-05-17T04:47:14.853875",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch3\"></a>\n",
    "# Step 1: Define the Problem\n",
    " \n",
    "\n",
    "**Project Summary:**\n",
    "Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n",
    "\n",
    "The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n",
    "\n",
    "While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n",
    "\n",
    "To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n",
    "\n",
    "Help save them and change history!\n",
    "\n",
    "Practice Skills\n",
    "* Binary classification\n",
    "* Python and R basics\n",
    "\n",
    "# Step 2: Gather the Data\n",
    "\n",
    "The dataset is also given to us on a golden plater with test and train data at [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic/data)\n"
   ],
   "metadata": {
    "_cell_guid": "22ba49b5-b3b6-4129-8c57-623e0d863ab3",
    "_uuid": "ff014109d795c5d1bfcc66c9761730406124ebe4",
    "papermill": {
     "duration": 0.027611,
     "end_time": "2023-05-17T04:47:14.937507",
     "exception": false,
     "start_time": "2023-05-17T04:47:14.909896",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch4\"></a>\n",
    "# Step 3: Prepare Data for Consumption\n",
    "Since step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n",
    "\n",
    "## 3.1 Import Libraries\n",
    "The following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. "
   ],
   "metadata": {
    "_cell_guid": "ef4059c6-9397-4b72-85da-570f51ba139c",
    "_uuid": "f1d8b59b37d2ce04a5396a0ab183dc3000817113",
    "papermill": {
     "duration": 0.028996,
     "end_time": "2023-05-17T04:47:14.994505",
     "exception": false,
     "start_time": "2023-05-17T04:47:14.965509",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Python 3.10 Best models Spaceship_Titanic program will you survive on the spaceship titanic or not\n",
    "File name Titanic_eda.py\n",
    "\n",
    "Version: 0.1\n",
    "Author: MLCV\n",
    "Date: 2023-08-01\n",
    "\"\"\"\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ],
   "metadata": {
    "_cell_guid": "a33744a8-08c7-4158-a22f-e8f2008a25b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "412b006d19b0209bd10ee4c6d15fdf59f8d2af38",
    "papermill": {
     "duration": 1.353462,
     "end_time": "2023-05-17T04:47:16.376316",
     "exception": false,
     "start_time": "2023-05-17T04:47:15.022854",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:19.925999Z",
     "iopub.execute_input": "2023-08-02T13:23:19.926440Z",
     "iopub.status.idle": "2023-08-02T13:23:20.673876Z",
     "shell.execute_reply.started": "2023-08-02T13:23:19.926410Z",
     "shell.execute_reply": "2023-08-02T13:23:20.672289Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.121992100Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.0 (default, Nov  6 2019, 21:49:08) \n",
      "[GCC 7.3.0]\n",
      "pandas version: 2.0.1\n",
      "matplotlib version: 3.7.1\n",
      "NumPy version: 1.23.5\n",
      "SciPy version: 1.10.1\n",
      "IPython version: 8.12.2\n",
      "scikit-learn version: 1.2.2\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../input': No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ls', '../input']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 52\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Input data files are available in the \"../input/\" directory.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msubprocess\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_output\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mcheck_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../input\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Any results you write to the current directory are saved as output.\u001B[39;00m\n",
      "File \u001B[0;32m/home/dske/anaconda3/envs/gluon_cuda110/lib/python3.8/subprocess.py:411\u001B[0m, in \u001B[0;36mcheck_output\u001B[0;34m(timeout, *popenargs, **kwargs)\u001B[0m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;66;03m# Explicitly passing input=None was previously equivalent to passing an\u001B[39;00m\n\u001B[1;32m    408\u001B[0m     \u001B[38;5;66;03m# empty string. That is maintained here for backwards compatibility.\u001B[39;00m\n\u001B[1;32m    409\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muniversal_newlines\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpopenargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m           \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstdout\n",
      "File \u001B[0;32m/home/dske/anaconda3/envs/gluon_cuda110/lib/python3.8/subprocess.py:512\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[1;32m    510\u001B[0m     retcode \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mpoll()\n\u001B[1;32m    511\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m check \u001B[38;5;129;01mand\u001B[39;00m retcode:\n\u001B[0;32m--> 512\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(retcode, process\u001B[38;5;241m.\u001B[39margs,\n\u001B[1;32m    513\u001B[0m                                  output\u001B[38;5;241m=\u001B[39mstdout, stderr\u001B[38;5;241m=\u001B[39mstderr)\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CompletedProcess(process\u001B[38;5;241m.\u001B[39margs, retcode, stdout, stderr)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command '['ls', '../input']' returned non-zero exit status 2."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.11 Load Data Modelling Libraries\n",
    "\n",
    "We will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
   ],
   "metadata": {
    "_cell_guid": "a514e15b-8caa-4b2e-bb46-13521761ef27",
    "_uuid": "2e4810f3c85ffdee7d7382167e455baf1c320bc8",
    "papermill": {
     "duration": 0.028826,
     "end_time": "2023-05-17T04:47:16.433935",
     "exception": false,
     "start_time": "2023-05-17T04:47:16.405109",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ],
   "metadata": {
    "_cell_guid": "b230c790-a3f5-46ed-b351-57e96cc8fa61",
    "_uuid": "4a16d09256317518769f21bf9cc38726df8b0078",
    "papermill": {
     "duration": 1.199177,
     "end_time": "2023-05-17T04:47:17.662808",
     "exception": false,
     "start_time": "2023-05-17T04:47:16.463631",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:20.676481Z",
     "iopub.execute_input": "2023-08-02T13:23:20.676915Z",
     "iopub.status.idle": "2023-08-02T13:23:22.087369Z",
     "shell.execute_reply.started": "2023-08-02T13:23:20.676876Z",
     "shell.execute_reply": "2023-08-02T13:23:22.086147Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.121992100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Meet and Greet Data\n",
    "\n",
    "This is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n",
    "\n",
    "To begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/competitions/spaceship-titanic/data).\n",
    "\n",
    "\n",
    "1. *PassengerId* - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "2. *HomePlanet* - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "3. *CryoSleep* - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "4. *Cabin* - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "5. *Destination* - The planet the passenger will be debarking to.\n",
    "6. *Age* - The age of the passenger.\n",
    "7. *VIP* - Whether the passenger has paid for special VIP service during the voyage.\n",
    "8. *RoomService, FoodCourt, ShoppingMall, Spa, VRDeck* - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "9. *Name* - The first and last names of the passenger.\n",
    "10. *Transported* - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ],
   "metadata": {
    "_cell_guid": "c6289058-157a-46ba-b5c3-f134b268f5cc",
    "_uuid": "a0fe0965de515e8c82cabd1fbd82cce993279f16",
    "papermill": {
     "duration": 0.028442,
     "end_time": "2023-05-17T04:47:17.721187",
     "exception": false,
     "start_time": "2023-05-17T04:47:17.692745",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "'''\n",
    "a dataset should be broken into 3 splits: train, test, and (final) validation\n",
    "the test file provided is the validation file for competition submission\n",
    "we will split the train set into train and test data in future sections\n",
    "'''\n",
    "data_raw = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n",
    "data_val  = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n",
    "\n",
    "\n",
    "# #to play with our data we'll create a copy\n",
    "# #remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "# #however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "#preview data\n",
    "print (data_raw.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "data_raw.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ],
   "metadata": {
    "_cell_guid": "c0928d7d-043b-4aa5-8113-e96fb42dbdc7",
    "_uuid": "d0afb79a714ea826208235d05afac4e3f60554db",
    "papermill": {
     "duration": 0.233236,
     "end_time": "2023-05-17T04:47:17.983408",
     "exception": false,
     "start_time": "2023-05-17T04:47:17.750172",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.088754Z",
     "iopub.execute_input": "2023-08-02T13:23:22.089112Z",
     "iopub.status.idle": "2023-08-02T13:23:22.295492Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.089084Z",
     "shell.execute_reply": "2023-08-02T13:23:22.294327Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.121992100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Duplicates\n",
    "print(f'Duplicates in train set: {data_raw.duplicated().sum()}, ({np.round(100*data_raw.duplicated().sum()/len(data_raw),1)}%)')\n",
    "print('')\n",
    "print(f'Duplicates in test set: {data_val.duplicated().sum()}, ({np.round(100*data_val.duplicated().sum()/len(data_val),1)}%)')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.095107,
     "end_time": "2023-05-17T04:47:18.108935",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.013828",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.296774Z",
     "iopub.execute_input": "2023-08-02T13:23:22.297105Z",
     "iopub.status.idle": "2023-08-02T13:23:22.370835Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.297077Z",
     "shell.execute_reply": "2023-08-02T13:23:22.369885Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.121992100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch5\"></a>\n",
    "## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\n",
    "In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n",
    "\n",
    "1. **Correction:** Reviewing the data did not reveal any erroneous or unacceptable inputs. However, we will wait for the completion of our exploratory analysis to determine whether we should include or exclude these outliers from the data set. It should be noted that if they were unreasonable values, like age = 800 instead of 80, then it's probably a safe solution to fix it now. However, we want to be careful when changing the data from its original value, as an accurate model may need to be created.\n",
    "2. **End:** in those fields where there are null values or no data. Missing values can be bad because some algorithms don't know how to handle null values and won't work. While others like decision trees can handle null values. Thus, it is important to correct before we start modeling because we will be comparing and contrasting several models. There are two common methods: either remove the entry or fill in the missing value using reasonable input. It is not recommended to delete an entry, especially a large percentage of entries, unless it really is an incomplete entry. Instead, it's better to impute missing values. The basic methodology for qualitative data is imputation using mode. The basic methodology for quantitative data is imputation using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use a basic methodology based on specific criteria; for example, average age by class or ship to port by fares and SES. There are more complex methodologies, but they should be compared to the base model before deployment to determine if complexity actually adds value. For this dataset, age will be imputed using the median, the cockpit attribute will be removed, and landing will be imputed using mode. Subsequent iterations of the model may change this decision to determine if it improves the accuracy of the model.\n",
    "3. **Create.** Feature development is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a header function to determine if it played a role in survival.\n",
    "4. **Transformation:** Last but not least, we'll get to the formatting. There are no date or currency formats, but there are data type formats. Our categorical data is imported as objects, making the math difficult. For this dataset, we will convert the feature data types to categorical dummy variables."
   ],
   "metadata": {
    "_cell_guid": "b575d2d8-b77d-4810-9232-3b2726145ca8",
    "_uuid": "281ad467b485855c1a39116c5ed899ac3018689f",
    "papermill": {
     "duration": 0.030046,
     "end_time": "2023-05-17T04:47:18.168392",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.138346",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ],
   "metadata": {
    "_cell_guid": "e2181a68-3c33-4040-83a9-99adee8363a0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "9a36681f0eda8ccc6396c64aa6bd6e8288461bcb",
    "papermill": {
     "duration": 0.146523,
     "end_time": "2023-05-17T04:47:18.345308",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.198785",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.373728Z",
     "iopub.execute_input": "2023-08-02T13:23:22.374315Z",
     "iopub.status.idle": "2023-08-02T13:23:22.509869Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.374285Z",
     "shell.execute_reply": "2023-08-02T13:23:22.508707Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_raw.nunique()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.06413,
     "end_time": "2023-05-17T04:47:18.440136",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.376006",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.511903Z",
     "iopub.execute_input": "2023-08-02T13:23:22.512803Z",
     "iopub.status.idle": "2023-08-02T13:23:22.536703Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.512759Z",
     "shell.execute_reply": "2023-08-02T13:23:22.535536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_raw.dtypes"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.045245,
     "end_time": "2023-05-17T04:47:18.517183",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.471938",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.538819Z",
     "iopub.execute_input": "2023-08-02T13:23:22.539790Z",
     "iopub.status.idle": "2023-08-02T13:23:22.550300Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.539690Z",
     "shell.execute_reply": "2023-08-02T13:23:22.548974Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 6 continuous features, 4 categorical features (excluding the target) and 3 descriptive/qualitative features."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.030961,
     "end_time": "2023-05-17T04:47:18.578938",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.547977",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Expenditure features\n",
    "exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Categorical features\n",
    "cat_feats=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "\n",
    "# Qualitative features\n",
    "qual_feats=['PassengerId', 'Cabin' ,'Name']"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.042339,
     "end_time": "2023-05-17T04:47:18.651576",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.609237",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.552393Z",
     "iopub.execute_input": "2023-08-02T13:23:22.552944Z",
     "iopub.status.idle": "2023-08-02T13:23:22.563817Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.552904Z",
     "shell.execute_reply": "2023-08-02T13:23:22.562385Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.22 Clean Data\n",
    "\n",
    "Now that we know what to clean, let's execute our code.\n",
    "\n",
    "** Developer Documentation: **\n",
    "* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
    "* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n",
    "* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n",
    "* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n",
    "* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n",
    "* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n",
    "* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n",
    "* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
    "* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n",
    "* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)"
   ],
   "metadata": {
    "_cell_guid": "c6fc9f66-8779-454c-b6bd-47676c0dc81c",
    "_uuid": "5002c97888cfdcb2b49ef76fd1505eed6a1721e4",
    "papermill": {
     "duration": 0.030217,
     "end_time": "2023-05-17T04:47:18.713346",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.683129",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner: \n",
    "    dataset['Age_group']=np.nan\n",
    "    dataset.loc[dataset['Age']<=12,'Age_group']='Age_0-12'\n",
    "    dataset.loc[(dataset['Age']>12) & (dataset['Age']<18),'Age_group']='Age_13-17'\n",
    "    dataset.loc[(dataset['Age']>=18) & (dataset['Age']<=25),'Age_group']='Age_18-25'\n",
    "    dataset.loc[(dataset['Age']>25) & (dataset['Age']<=30),'Age_group']='Age_26-30'\n",
    "    dataset.loc[(dataset['Age']>30) & (dataset['Age']<=50),'Age_group']='Age_31-50'\n",
    "    dataset.loc[dataset['Age']>50,'Age_group']='Age_51+'\n",
    "\n",
    "# Plot distribution of new features\n",
    "plt.figure(figsize=(10,4))\n",
    "g=sns.countplot(data=data1, x='Age_group', hue='Transported', order=['Age_0-12','Age_13-17','Age_18-25','Age_26-30','Age_31-50','Age_51+'])\n",
    "plt.title('Age group distribution')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.490244,
     "end_time": "2023-05-17T04:47:19.234843",
     "exception": false,
     "start_time": "2023-05-17T04:47:18.744599",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:22.566257Z",
     "iopub.execute_input": "2023-08-02T13:23:22.566837Z",
     "iopub.status.idle": "2023-08-02T13:23:23.153883Z",
     "shell.execute_reply.started": "2023-08-02T13:23:22.566796Z",
     "shell.execute_reply": "2023-08-02T13:23:23.152541Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner: \n",
    "    dataset['Expenditure']=dataset[exp_feats].sum(axis=1)\n",
    "    dataset['No_spending']=(dataset['Expenditure']==0).astype(int)\n",
    "\n",
    "# Plot distribution of new features\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(data=data1, x='Expenditure', hue='Transported', bins=200)\n",
    "plt.title('Total expenditure (truncated)')\n",
    "plt.ylim([0,200])\n",
    "plt.xlim([0,20000])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.countplot(data=data1, x='No_spending', hue='Transported')\n",
    "plt.title('No spending indicator')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "papermill": {
     "duration": 2.201429,
     "end_time": "2023-05-17T04:47:21.468045",
     "exception": false,
     "start_time": "2023-05-17T04:47:19.266616",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:23.155245Z",
     "iopub.execute_input": "2023-08-02T13:23:23.155683Z",
     "iopub.status.idle": "2023-08-02T13:23:25.443730Z",
     "shell.execute_reply.started": "2023-08-02T13:23:23.155643Z",
     "shell.execute_reply": "2023-08-02T13:23:25.441966Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.200138400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner: \n",
    "    dataset['Group'] = dataset['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "    dataset['Group_size']=dataset['Group'].map(lambda x: dataset['Group'].value_counts()[x])\n",
    "\n",
    "# Plot distribution of new features\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(data=data1, x='Group', hue='Transported', binwidth=1)\n",
    "plt.title('Group')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.countplot(data=data1, x='Group_size', hue='Transported')\n",
    "plt.title('Group size')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "papermill": {
     "duration": 73.242489,
     "end_time": "2023-05-17T04:48:34.742845",
     "exception": false,
     "start_time": "2023-05-17T04:47:21.500356",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:23:25.445667Z",
     "iopub.execute_input": "2023-08-02T13:23:25.447046Z",
     "iopub.status.idle": "2023-08-02T13:24:32.722484Z",
     "shell.execute_reply.started": "2023-08-02T13:23:25.446998Z",
     "shell.execute_reply": "2023-08-02T13:24:32.720893Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.325151700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "We can't really use the Group feature in our models because it has too big of a cardinality (6217) and \n",
    "would explode the number of dimensions with one-hot encoding.\n",
    "\n",
    "The Group size on the other hand should be a useful feature. In fact, we can compress the feature further by \n",
    "creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right \n",
    "shows that group size=1 is less likely to be transported than group size>1.\n",
    "'''\n",
    "\n",
    "\n",
    "# # New feature\n",
    "# data_raw['Solo']=(data_raw['Group_size']==1).astype(int)\n",
    "# data_val['Solo']=(data_val['Group_size']==1).astype(int)\n",
    "\n",
    "for dataset in data_cleaner: \n",
    "    dataset['Solo']=(dataset['Group_size']==1).astype(int)\n",
    "    \n",
    "\n",
    "# New feature distribution\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=data1, x='Solo', hue='Transported')\n",
    "plt.title('Passenger travelling solo or not')\n",
    "plt.ylim([0,3000])"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.352625,
     "end_time": "2023-05-17T04:48:35.128121",
     "exception": false,
     "start_time": "2023-05-17T04:48:34.775496",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:24:32.724742Z",
     "iopub.execute_input": "2023-08-02T13:24:32.725174Z",
     "iopub.status.idle": "2023-08-02T13:24:33.086133Z",
     "shell.execute_reply.started": "2023-08-02T13:24:32.725135Z",
     "shell.execute_reply": "2023-08-02T13:24:33.084645Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner: \n",
    "    # Replace NaN's with outliers for now (so we can split feature)\n",
    "    dataset['Cabin'].fillna('Z/9999/Z', inplace=True)\n",
    "    # New features\n",
    "    dataset['Cabin_deck'] = dataset['Cabin'].apply(lambda x: x.split('/')[0])\n",
    "    dataset['Cabin_number'] = dataset['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "    dataset['Cabin_side'] = dataset['Cabin'].apply(lambda x: x.split('/')[2])\n",
    "    \n",
    "    # Put Nan's back in (we will fill these later)\n",
    "    dataset.loc[dataset['Cabin_deck']=='Z', 'Cabin_deck']=np.nan\n",
    "    dataset.loc[dataset['Cabin_number']==9999, 'Cabin_number']=np.nan\n",
    "    dataset.loc[dataset['Cabin_side']=='Z', 'Cabin_side']=np.nan\n",
    "    \n",
    "    # Drop Cabin (we don't need it anymore)\n",
    "    dataset.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Plot distribution of new features\n",
    "fig=plt.figure(figsize=(10,12))\n",
    "plt.subplot(3,1,1)\n",
    "sns.countplot(data=data1, x='Cabin_deck', hue='Transported', order=['A','B','C','D','E','F','G','T'])\n",
    "plt.title('Cabin deck')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "sns.histplot(data=data1, x='Cabin_number', hue='Transported',binwidth=20)\n",
    "plt.vlines(300, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(600, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(900, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1200, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1500, ymin=0, ymax=200, color='black')\n",
    "plt.vlines(1800, ymin=0, ymax=200, color='black')\n",
    "plt.title('Cabin number')\n",
    "plt.xlim([0,2000])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "sns.countplot(data=data1, x='Cabin_side', hue='Transported')\n",
    "plt.title('Cabin side')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.82568,
     "end_time": "2023-05-17T04:48:36.987229",
     "exception": false,
     "start_time": "2023-05-17T04:48:35.161549",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:24:33.087881Z",
     "iopub.execute_input": "2023-08-02T13:24:33.088268Z",
     "iopub.status.idle": "2023-08-02T13:24:35.079970Z",
     "shell.execute_reply.started": "2023-08-02T13:24:33.088235Z",
     "shell.execute_reply": "2023-08-02T13:24:35.078653Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner:\n",
    "    # New features - training set\n",
    "    dataset['Cabin_region1']=(dataset['Cabin_number']<300).astype(int)   # one-hot encoding\n",
    "    dataset['Cabin_region2']=((dataset['Cabin_number']>=300) & (dataset['Cabin_number']<600)).astype(int)\n",
    "    dataset['Cabin_region3']=((dataset['Cabin_number']>=600) & (dataset['Cabin_number']<900)).astype(int)\n",
    "    dataset['Cabin_region4']=((dataset['Cabin_number']>=900) & (dataset['Cabin_number']<1200)).astype(int)\n",
    "    dataset['Cabin_region5']=((dataset['Cabin_number']>=1200) & (dataset['Cabin_number']<1500)).astype(int)\n",
    "    dataset['Cabin_region6']=((dataset['Cabin_number']>=1500) & (dataset['Cabin_number']<1800)).astype(int)\n",
    "    dataset['Cabin_region7']=(dataset['Cabin_number']>=1800).astype(int)\n",
    "\n",
    "# Plot distribution of new features\n",
    "plt.figure(figsize=(10,4))\n",
    "data1['Cabin_regions_plot']=(data1['Cabin_region1']+2*data1['Cabin_region2']+3*data1['Cabin_region3']+4*data1['Cabin_region4']+5*data1['Cabin_region5']+6*data1['Cabin_region6']+7*data1['Cabin_region7']).astype(int)\n",
    "sns.countplot(data=data1, x='Cabin_regions_plot', hue='Transported')\n",
    "plt.title('Cabin regions')\n",
    "data1.drop('Cabin_regions_plot', axis=1, inplace=True)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.468977,
     "end_time": "2023-05-17T04:48:37.491466",
     "exception": false,
     "start_time": "2023-05-17T04:48:37.022489",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:24:35.088146Z",
     "iopub.execute_input": "2023-08-02T13:24:35.089589Z",
     "iopub.status.idle": "2023-08-02T13:24:35.601157Z",
     "shell.execute_reply.started": "2023-08-02T13:24:35.089546Z",
     "shell.execute_reply": "2023-08-02T13:24:35.599633Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner:\n",
    "    # Calculate family size from last name.\n",
    "    # Replace NaN's with outliers for now (so we can split feature)\n",
    "    dataset['Name'].fillna('Unknown Unknown', inplace=True)\n",
    "    # New feature - Surname\n",
    "    dataset['Surname']=dataset['Name'].str.split().str[-1]\n",
    "    # New feature - Family size\n",
    "    dataset['Family_size']=dataset['Surname'].map(lambda x: dataset['Surname'].value_counts()[x])\n",
    "    # Put Nan's back in (we will fill these later)\n",
    "    dataset.loc[dataset['Surname']=='Unknown','Surname']=np.nan\n",
    "    dataset.loc[dataset['Family_size']>100,'Family_size']=np.nan\n",
    "    # Drop name (we don't need it anymore)\n",
    "    dataset.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# New feature distribution\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(data=data1, x='Family_size', hue='Transported')\n",
    "plt.title('Family size')"
   ],
   "metadata": {
    "papermill": {
     "duration": 37.845602,
     "end_time": "2023-05-17T04:49:15.372109",
     "exception": false,
     "start_time": "2023-05-17T04:48:37.526507",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:24:35.602946Z",
     "iopub.execute_input": "2023-08-02T13:24:35.603295Z",
     "iopub.status.idle": "2023-08-02T13:25:11.127527Z",
     "shell.execute_reply.started": "2023-08-02T13:24:35.603267Z",
     "shell.execute_reply": "2023-08-02T13:25:11.125897Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing values"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.035619,
     "end_time": "2023-05-17T04:49:15.443849",
     "exception": false,
     "start_time": "2023-05-17T04:49:15.40823",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data1['Transported'].astype(int)\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # Columns with missing values\n",
    "    na_cols=dataset.columns[dataset.isna().any()].tolist()\n",
    "    mv=pd.DataFrame(dataset[na_cols].isna().sum(), columns=['Number_missing'])\n",
    "    mv['Percentage_missing']=np.round(100*mv['Number_missing']/len(dataset),2)\n",
    "    print(mv, '\\n')\n",
    "    "
   ],
   "metadata": {
    "_cell_guid": "56e3e982-a23a-4c84-b8a1-976adc94aa0b",
    "_uuid": "160dd59e4ebe7a94521c65da2746ca370b4bd694",
    "papermill": {
     "duration": 0.093662,
     "end_time": "2023-05-17T04:49:15.573126",
     "exception": false,
     "start_time": "2023-05-17T04:49:15.479464",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:11.129253Z",
     "iopub.execute_input": "2023-08-02T13:25:11.130409Z",
     "iopub.status.idle": "2023-08-02T13:25:11.233946Z",
     "shell.execute_reply.started": "2023-08-02T13:25:11.130371Z",
     "shell.execute_reply": "2023-08-02T13:25:11.232521Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Countplot of number of missing values by passenger\n",
    "data1['na_count']=data1.isna().sum(axis=1)\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=data1, x='na_count', hue='Transported')\n",
    "plt.title('Number of missing entries by passenger')\n",
    "data1.drop('na_count', axis=1, inplace=True)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.432142,
     "end_time": "2023-05-17T04:49:16.040879",
     "exception": false,
     "start_time": "2023-05-17T04:49:15.608737",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:11.235236Z",
     "iopub.execute_input": "2023-08-02T13:25:11.235574Z",
     "iopub.status.idle": "2023-08-02T13:25:11.709058Z",
     "shell.execute_reply.started": "2023-08-02T13:25:11.235547Z",
     "shell.execute_reply": "2023-08-02T13:25:11.707650Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.356394500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "source": [
    "Notes:\n",
    "\n",
    "Missing values are independent of the target and for the most part are isolated.\n",
    "Even though only 2% of the data is missing, about 25% of all passengers have at least 1 missing value.\n",
    "PassengerId is the only (original) feature to not have any missing values.\n",
    "Insight:\n",
    "\n",
    "Since most of the missing values are isolated it makes sense to try to fill these in as opposed to just dropping rows.\n",
    "If there is a relationship between PassengerId and other features we can fill missing values according to this column.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "The easiest way to deal with missing values is to just use the median for continuous features and the mode for categorical features. This will work 'well enough' but if we want to maximise the accuracy of our models then we need to look for patterns within the missing data. The way to do this is by looking at the joint distribution of features, e.g. do passengers from the same group tend to come from the same family? There are obviously many combinations so we will just summarise the useful trends I and others have found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in data_cleaner:\n",
    "    GHP_gb=dataset.groupby(['Group','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    HP_bef=dataset['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Passengers with missing HomePlanet and in a group with known HomePlanet\n",
    "    GHP_index=dataset[dataset['HomePlanet'].isna()][(dataset[dataset['HomePlanet'].isna()]['Group']).isin(GHP_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    dataset.loc[GHP_index,'HomePlanet']=dataset.iloc[GHP_index,:]['Group'].map(lambda x: GHP_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',dataset['HomePlanet'].isna().sum())\n",
    "\n",
    "# We managed to fill 131 values with 100% confidence but we are not finished yet."
   ],
   "metadata": {
    "papermill": {
     "duration": 1.504027,
     "end_time": "2023-05-17T04:49:17.65437",
     "exception": false,
     "start_time": "2023-05-17T04:49:16.150343",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:11.710375Z",
     "iopub.execute_input": "2023-08-02T13:25:11.711261Z",
     "iopub.status.idle": "2023-08-02T13:25:13.022801Z",
     "shell.execute_reply.started": "2023-08-02T13:25:11.711201Z",
     "shell.execute_reply": "2023-08-02T13:25:13.020940Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Notes:\n",
    "\n",
    "Passengers on decks A, B, C or T came from Europa.\n",
    "Passengers on deck G came from Earth.\n",
    "Passengers on decks D, E or F came from multiple planets.\n",
    "'''\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    # Missing values before\n",
    "    HP_bef=dataset['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Decks A, B, C or T came from Europa\n",
    "    dataset.loc[(dataset['HomePlanet'].isna()) & (dataset['Cabin_deck'].isin(['A', 'B', 'C', 'T'])), 'HomePlanet']='Europa'\n",
    "\n",
    "    # Deck G came from Earth\n",
    "    dataset.loc[(dataset['HomePlanet'].isna()) & (dataset['Cabin_deck']=='G'), 'HomePlanet']='Earth'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',dataset['HomePlanet'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.071138,
     "end_time": "2023-05-17T04:49:17.762254",
     "exception": false,
     "start_time": "2023-05-17T04:49:17.691116",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:13.024924Z",
     "iopub.execute_input": "2023-08-02T13:25:13.025338Z",
     "iopub.status.idle": "2023-08-02T13:25:13.050265Z",
     "shell.execute_reply.started": "2023-08-02T13:25:13.025308Z",
     "shell.execute_reply": "2023-08-02T13:25:13.048914Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    SHP_gb=data.groupby(['Surname','HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "    #  Everyone with the same surname comes from the same home planet.\n",
    "    # Missing values before\n",
    "    HP_bef=data['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Passengers with missing HomePlanet and in a family with known HomePlanet\n",
    "    SHP_index=data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['Surname']).isin(SHP_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[SHP_index,'HomePlanet']=data.iloc[SHP_index,:]['Surname'].map(lambda x: SHP_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.464308,
     "end_time": "2023-05-17T04:49:18.266601",
     "exception": false,
     "start_time": "2023-05-17T04:49:17.802293",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:13.052338Z",
     "iopub.execute_input": "2023-08-02T13:25:13.052848Z",
     "iopub.status.idle": "2023-08-02T13:25:13.447204Z",
     "shell.execute_reply.started": "2023-08-02T13:25:13.052803Z",
     "shell.execute_reply": "2023-08-02T13:25:13.445552Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    HP_bef=data['HomePlanet'].isna().sum()\n",
    "\n",
    "    # Fill remaining HomePlanet missing values with Earth (if not on deck D) or Mars (if on Deck D)\n",
    "    data.loc[(data['HomePlanet'].isna()) & ~(data['Cabin_deck']=='D'), 'HomePlanet']='Earth'\n",
    "    data.loc[(data['HomePlanet'].isna()) & (data['Cabin_deck']=='D'), 'HomePlanet']='Mars'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#HomePlanet missing values before:',HP_bef)\n",
    "    print('#HomePlanet missing values after:',data['HomePlanet'].isna().sum())\n",
    "\n",
    "# We're done with HomePlanet."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.06871,
     "end_time": "2023-05-17T04:49:18.372163",
     "exception": false,
     "start_time": "2023-05-17T04:49:18.303453",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:13.449048Z",
     "iopub.execute_input": "2023-08-02T13:25:13.449909Z",
     "iopub.status.idle": "2023-08-02T13:25:13.476587Z",
     "shell.execute_reply.started": "2023-08-02T13:25:13.449867Z",
     "shell.execute_reply": "2023-08-02T13:25:13.475191Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    D_bef=data['Destination'].isna().sum()\n",
    "\n",
    "    # Fill missing Destination values with mode\n",
    "    data.loc[(data['Destination'].isna()), 'Destination']='TRAPPIST-1e'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Destination missing values before:',D_bef)\n",
    "    print('#Destination missing values after:',data['Destination'].isna().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution of Group and Surname\n",
    "    GSN_gb=data[data['Group_size']>1].groupby(['Group','Surname'])['Surname'].size().unstack().fillna(0)\n",
    "\n",
    "    # The majority (83%) of groups contain only 1 family. So let's fill missing surnames according to the majority surname in that group.\n",
    "    # Missing values before\n",
    "    SN_bef=data['Surname'].isna().sum()\n",
    "\n",
    "    # Passengers with missing Surname and in a group with known majority Surname\n",
    "    GSN_index=data[data['Surname'].isna()][(data[data['Surname'].isna()]['Group']).isin(GSN_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[GSN_index,'Surname']=data.iloc[GSN_index,:]['Group'].map(lambda x: GSN_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Surname missing values before:',SN_bef)\n",
    "    print('#Surname missing values after:',data['Surname'].isna().sum())\n",
    "\n",
    "    # Replace NaN's with outliers (so we can use map)\n",
    "    data['Surname'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Update family size feature\n",
    "    data['Family_size']=data['Surname'].map(lambda x: data['Surname'].value_counts()[x])\n",
    "\n",
    "    # Put NaN's back in place of outliers\n",
    "    data.loc[data['Surname']=='Unknown','Surname']=np.nan\n",
    "\n",
    "    # Say unknown surname means no family\n",
    "    data.loc[data['Family_size']>100,'Family_size']=0"
   ],
   "metadata": {
    "papermill": {
     "duration": 40.309967,
     "end_time": "2023-05-17T04:49:58.81947",
     "exception": false,
     "start_time": "2023-05-17T04:49:18.509503",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:13.499181Z",
     "iopub.execute_input": "2023-08-02T13:25:13.499742Z",
     "iopub.status.idle": "2023-08-02T13:25:49.766540Z",
     "shell.execute_reply.started": "2023-08-02T13:25:13.499699Z",
     "shell.execute_reply": "2023-08-02T13:25:49.765213Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution of Group and Cabin features\n",
    "    GCD_gb=data[data['Group_size']>1].groupby(['Group','Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)\n",
    "    GCN_gb=data[data['Group_size']>1].groupby(['Group','Cabin_number'])['Cabin_number'].size().unstack().fillna(0)\n",
    "    GCS_gb=data[data['Group_size']>1].groupby(['Group','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n",
    "\n",
    "    # Everyone in the same group is also on the same cabin side. For cabin deck and cabin number there is also a fairly good (but not perfect) correlation with group.\n",
    "    # Missing values before\n",
    "    CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "    # Passengers with missing Cabin side and in a group with known Cabin side\n",
    "    GCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Group']).isin(GCS_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[GCS_index,'Cabin_side']=data.iloc[GCS_index,:]['Group'].map(lambda x: GCS_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.580189,
     "end_time": "2023-05-17T04:49:59.438155",
     "exception": false,
     "start_time": "2023-05-17T04:49:58.857966",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:49.767971Z",
     "iopub.execute_input": "2023-08-02T13:25:49.768298Z",
     "iopub.status.idle": "2023-08-02T13:25:50.245343Z",
     "shell.execute_reply.started": "2023-08-02T13:25:49.768271Z",
     "shell.execute_reply": "2023-08-02T13:25:50.243233Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution of Surname and Cabin side\n",
    "    SCS_gb=data[data['Group_size']>1].groupby(['Surname','Cabin_side'])['Cabin_side'].size().unstack().fillna(0)\n",
    "\n",
    "    # Ratio of sides\n",
    "    SCS_gb['Ratio']=SCS_gb['P']/(SCS_gb['P']+SCS_gb['S'])\n",
    "\n",
    "    # Histogram of ratio\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.histplot(SCS_gb['Ratio'], kde=True, binwidth=0.05)\n",
    "    plt.title('Ratio of cabin side by surname')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.88369,
     "end_time": "2023-05-17T04:50:00.360314",
     "exception": false,
     "start_time": "2023-05-17T04:49:59.476624",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:50.247711Z",
     "iopub.execute_input": "2023-08-02T13:25:50.248483Z",
     "iopub.status.idle": "2023-08-02T13:25:51.184138Z",
     "shell.execute_reply.started": "2023-08-02T13:25:50.248416Z",
     "shell.execute_reply": "2023-08-02T13:25:51.182986Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.512093200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Print proportion\n",
    "print('Percentage of families all on the same cabin side:', 100*np.round((SCS_gb['Ratio'].isin([0,1])).sum()/len(SCS_gb),3),'%')\n",
    "\n",
    "# Another view of the same information\n",
    "SCS_gb.head()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.069169,
     "end_time": "2023-05-17T04:50:00.46909",
     "exception": false,
     "start_time": "2023-05-17T04:50:00.399921",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.185661Z",
     "iopub.execute_input": "2023-08-02T13:25:51.185987Z",
     "iopub.status.idle": "2023-08-02T13:25:51.204439Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.185959Z",
     "shell.execute_reply": "2023-08-02T13:25:51.202910Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Missing values before\n",
    "CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "# Drop ratio column\n",
    "SCS_gb.drop('Ratio', axis=1, inplace=True)\n",
    "    \n",
    "for data in data_cleaner:\n",
    "    # Passengers with missing Cabin side and in a family with known Cabin side\n",
    "    SCS_index=data[data['Cabin_side'].isna()][(data[data['Cabin_side'].isna()]['Surname']).isin(SCS_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[SCS_index,'Cabin_side']=data.iloc[SCS_index,:]['Surname'].map(lambda x: SCS_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Drop surname (we don't need it anymore)\n",
    "    data.drop('Surname', axis=1, inplace=True)\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.183491,
     "end_time": "2023-05-17T04:50:00.692541",
     "exception": false,
     "start_time": "2023-05-17T04:50:00.50905",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.206526Z",
     "iopub.execute_input": "2023-08-02T13:25:51.207101Z",
     "iopub.status.idle": "2023-08-02T13:25:51.327104Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.207053Z",
     "shell.execute_reply": "2023-08-02T13:25:51.325317Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Value counts\n",
    "    data['Cabin_side'].value_counts()\n",
    "\n",
    "    # Missing values before\n",
    "    CS_bef=data['Cabin_side'].isna().sum()\n",
    "\n",
    "    # Fill remaining missing values with outlier\n",
    "    data.loc[data['Cabin_side'].isna(),'Cabin_side']='Z'\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_side missing values before:',CS_bef)\n",
    "    print('#Cabin_side missing values after:',data['Cabin_side'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.067021,
     "end_time": "2023-05-17T04:50:00.798936",
     "exception": false,
     "start_time": "2023-05-17T04:50:00.731915",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.328948Z",
     "iopub.execute_input": "2023-08-02T13:25:51.329327Z",
     "iopub.status.idle": "2023-08-02T13:25:51.348579Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.329295Z",
     "shell.execute_reply": "2023-08-02T13:25:51.346961Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    CD_bef=data['Cabin_deck'].isna().sum()\n",
    "\n",
    "    # Passengers with missing Cabin deck and in a group with known majority Cabin deck\n",
    "    GCD_index=data[data['Cabin_deck'].isna()][(data[data['Cabin_deck'].isna()]['Group']).isin(GCD_gb.index)].index\n",
    "\n",
    "    # Fill corresponding missing values\n",
    "    data.loc[GCD_index,'Cabin_deck']=data.iloc[GCD_index,:]['Group'].map(lambda x: GCD_gb.idxmax(axis=1)[x])\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_deck missing values before:',CD_bef)\n",
    "    print('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.161046,
     "end_time": "2023-05-17T04:50:00.999999",
     "exception": false,
     "start_time": "2023-05-17T04:50:00.838953",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.350142Z",
     "iopub.execute_input": "2023-08-02T13:25:51.350582Z",
     "iopub.status.idle": "2023-08-02T13:25:51.449471Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.350538Z",
     "shell.execute_reply": "2023-08-02T13:25:51.448322Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet','Destination','Solo','Cabin_deck'])['Cabin_deck'].size().unstack().fillna(0)\n",
    "    '''\n",
    "    Passengers from Mars are most likely in deck F.\n",
    "    Passengers from Europa are (more or less) most likely in deck C if travelling solo and deck B otherwise.\n",
    "    Passengers from Earth are (more or less) most likely in deck G.\n",
    "    We will fill in missing values according to where the mode appears in these subgroups.\n",
    "    '''\n",
    "    # Missing values before\n",
    "    CD_bef=data['Cabin_deck'].isna().sum()\n",
    "\n",
    "    # Fill missing values using the mode\n",
    "    na_rows_CD=data.loc[data['Cabin_deck'].isna(),'Cabin_deck'].index\n",
    "    data.loc[data['Cabin_deck'].isna(),'Cabin_deck']=data.groupby(['HomePlanet','Destination','Solo'])['Cabin_deck'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CD]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_deck missing values before:',CD_bef)\n",
    "    print('#Cabin_deck missing values after:',data['Cabin_deck'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.118815,
     "end_time": "2023-05-17T04:50:01.159875",
     "exception": false,
     "start_time": "2023-05-17T04:50:01.04106",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.450962Z",
     "iopub.execute_input": "2023-08-02T13:25:51.451303Z",
     "iopub.status.idle": "2023-08-02T13:25:51.525347Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.451274Z",
     "shell.execute_reply": "2023-08-02T13:25:51.524281Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Scatterplot\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.scatterplot(x=data['Cabin_number'], y=data['Group'], c=LabelEncoder().fit_transform(data.loc[~data['Cabin_number'].isna(),'Cabin_deck']), cmap='tab10')\n",
    "    plt.title('Cabin_number vs group coloured by group ')"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.338864,
     "end_time": "2023-05-17T04:50:02.538837",
     "exception": false,
     "start_time": "2023-05-17T04:50:01.199973",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:51.527021Z",
     "iopub.execute_input": "2023-08-02T13:25:51.527384Z",
     "iopub.status.idle": "2023-08-02T13:25:52.977876Z",
     "shell.execute_reply.started": "2023-08-02T13:25:51.527356Z",
     "shell.execute_reply": "2023-08-02T13:25:52.976496Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    CN_bef=data['Cabin_number'].isna().sum()\n",
    "    print('#Cabin_number missing values before:',CN_bef)\n",
    "    # Extrapolate linear relationship on a deck by deck basis\n",
    "    for deck in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        # Features and labels\n",
    "        X_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n",
    "        y_CN=data.loc[~(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']\n",
    "        X_test_CN=data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Group']\n",
    "\n",
    "        if not X_test_CN.empty:\n",
    "            # Linear regression\n",
    "            model_CN=sklearn.linear_model.LinearRegression()\n",
    "            model_CN.fit(X_CN.values.reshape(-1, 1), y_CN)\n",
    "            preds_CN=model_CN.predict(X_test_CN.values.reshape(-1, 1))\n",
    "\n",
    "            # Fill missing values with predictions\n",
    "            data.loc[(data['Cabin_number'].isna()) & (data['Cabin_deck']==deck),'Cabin_number']=preds_CN.astype(int)\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Cabin_number missing values before:',CN_bef)\n",
    "    print('#Cabin_number missing values after:',data['Cabin_number'].isna().sum())\n",
    "\n",
    "    # One-hot encode cabin regions\n",
    "    data['Cabin_region1']=(data['Cabin_number']<300).astype(int)\n",
    "    data['Cabin_region2']=((data['Cabin_number']>=300) & (data['Cabin_number']<600)).astype(int)\n",
    "    data['Cabin_region3']=((data['Cabin_number']>=600) & (data['Cabin_number']<900)).astype(int)\n",
    "    data['Cabin_region4']=((data['Cabin_number']>=900) & (data['Cabin_number']<1200)).astype(int)\n",
    "    data['Cabin_region5']=((data['Cabin_number']>=1200) & (data['Cabin_number']<1500)).astype(int)\n",
    "    data['Cabin_region6']=((data['Cabin_number']>=1500) & (data['Cabin_number']<1800)).astype(int)\n",
    "    data['Cabin_region7']=(data['Cabin_number']>=1800).astype(int)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.218567,
     "end_time": "2023-05-17T04:50:02.805649",
     "exception": false,
     "start_time": "2023-05-17T04:50:02.587082",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:52.980195Z",
     "iopub.execute_input": "2023-08-02T13:25:52.981173Z",
     "iopub.status.idle": "2023-08-02T13:25:53.140182Z",
     "shell.execute_reply.started": "2023-08-02T13:25:52.981126Z",
     "shell.execute_reply": "2023-08-02T13:25:53.138538Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    V_bef=data['VIP'].isna().sum()\n",
    "\n",
    "    # Fill missing values with mode\n",
    "    data.loc[data['VIP'].isna(),'VIP']=False\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#VIP missing values before:',V_bef)\n",
    "    print('#VIP missing values after:',data['VIP'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.077016,
     "end_time": "2023-05-17T04:50:02.94702",
     "exception": false,
     "start_time": "2023-05-17T04:50:02.870004",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.142223Z",
     "iopub.execute_input": "2023-08-02T13:25:53.142637Z",
     "iopub.status.idle": "2023-08-02T13:25:53.161969Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.142605Z",
     "shell.execute_reply": "2023-08-02T13:25:53.161016Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].median().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    A_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # Fill missing values using the median\n",
    "    na_rows_A=data.loc[data['Age'].isna(),'Age'].index\n",
    "    data.loc[data['Age'].isna(),'Age']=data.groupby(['HomePlanet','No_spending','Solo','Cabin_deck'])['Age'].transform(lambda x: x.fillna(x.median()))[na_rows_A]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Age missing values before:',A_bef)\n",
    "    print('#Age missing values after:',data['Age'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.138315,
     "end_time": "2023-05-17T04:50:03.130834",
     "exception": false,
     "start_time": "2023-05-17T04:50:02.992519",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.163509Z",
     "iopub.execute_input": "2023-08-02T13:25:53.164482Z",
     "iopub.status.idle": "2023-08-02T13:25:53.262773Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.164394Z",
     "shell.execute_reply": "2023-08-02T13:25:53.260672Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Update age group feature\n",
    "    data.loc[data['Age']<=12,'Age_group']='Age_0-12'\n",
    "    data.loc[(data['Age']>12) & (data['Age']<18),'Age_group']='Age_13-17'\n",
    "    data.loc[(data['Age']>=18) & (data['Age']<=25),'Age_group']='Age_18-25'\n",
    "    data.loc[(data['Age']>25) & (data['Age']<=30),'Age_group']='Age_26-30'\n",
    "    data.loc[(data['Age']>30) & (data['Age']<=50),'Age_group']='Age_31-50'\n",
    "    data.loc[data['Age']>50,'Age_group']='Age_51+'"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.073202,
     "end_time": "2023-05-17T04:50:03.252086",
     "exception": false,
     "start_time": "2023-05-17T04:50:03.178884",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.264574Z",
     "iopub.execute_input": "2023-08-02T13:25:53.264931Z",
     "iopub.status.idle": "2023-08-02T13:25:53.286965Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.264902Z",
     "shell.execute_reply": "2023-08-02T13:25:53.285245Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['No_spending','CryoSleep'])['CryoSleep'].size().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    CSL_bef=data['CryoSleep'].isna().sum()\n",
    "\n",
    "    # Fill missing values using the mode\n",
    "    na_rows_CSL=data.loc[data['CryoSleep'].isna(),'CryoSleep'].index\n",
    "    data.loc[data['CryoSleep'].isna(),'CryoSleep']=data.groupby(['No_spending'])['CryoSleep'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[na_rows_CSL]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#CryoSleep missing values before:',CSL_bef)\n",
    "    print('#CryoSleep missing values after:',data['CryoSleep'].isna().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.095097,
     "end_time": "2023-05-17T04:50:03.39283",
     "exception": false,
     "start_time": "2023-05-17T04:50:03.297733",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.288713Z",
     "iopub.execute_input": "2023-08-02T13:25:53.289044Z",
     "iopub.status.idle": "2023-08-02T13:25:53.336860Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.289018Z",
     "shell.execute_reply": "2023-08-02T13:25:53.335499Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Missing values before\n",
    "    E_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # CryoSleep has no expenditure\n",
    "    for col in exp_feats:\n",
    "        data.loc[(data[col].isna()) & (data['CryoSleep']==True), col]=0\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Expenditure missing values before:',E_bef)\n",
    "    print('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.090443,
     "end_time": "2023-05-17T04:50:03.530333",
     "exception": false,
     "start_time": "2023-05-17T04:50:03.43989",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.338697Z",
     "iopub.execute_input": "2023-08-02T13:25:53.339152Z",
     "iopub.status.idle": "2023-08-02T13:25:53.382889Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.339109Z",
     "shell.execute_reply": "2023-08-02T13:25:53.381458Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-08-08T08:47:05.950180900Z",
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_cleaner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdata_cleaner\u001B[49m:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Missing values before\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     E_bef\u001B[38;5;241m=\u001B[39mdata[exp_feats]\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# CryoSleep has no expenditure\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data_cleaner' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Joint distribution\n",
    "    data.groupby(['HomePlanet','Solo','Age_group'])['Expenditure'].mean().unstack().fillna(0)\n",
    "    # Missing values before\n",
    "    E_bef=data[exp_feats].isna().sum().sum()\n",
    "\n",
    "    # Fill remaining missing values using the median\n",
    "    for col in exp_feats:\n",
    "        na_rows=data.loc[data[col].isna(),col].index\n",
    "        data.loc[data[col].isna(),col]=data.groupby(['HomePlanet','Solo','Age_group'])[col].transform(lambda x: x.fillna(x.mean()))[na_rows]\n",
    "\n",
    "    # Print number of missing values left\n",
    "    print('#Expenditure missing values before:',E_bef)\n",
    "    print('#Expenditure missing values after:',data[exp_feats].isna().sum().sum())"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.270298,
     "end_time": "2023-05-17T04:50:03.846596",
     "exception": false,
     "start_time": "2023-05-17T04:50:03.576298",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.384644Z",
     "iopub.execute_input": "2023-08-02T13:25:53.385009Z",
     "iopub.status.idle": "2023-08-02T13:25:53.641695Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.384978Z",
     "shell.execute_reply": "2023-08-02T13:25:53.640229Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for data in data_cleaner:\n",
    "    # Update expenditure and no_spending\n",
    "    data['Expenditure']=data[exp_feats].sum(axis=1)\n",
    "    data['No_spending']=(data['Expenditure']==0).astype(int)\n",
    "    data.isna().sum()\n",
    "    # Apply log transform\n",
    "    for col in ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','Expenditure']:\n",
    "        data[col]=np.log(1+data[col])"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.086291,
     "end_time": "2023-05-17T04:50:03.979176",
     "exception": false,
     "start_time": "2023-05-17T04:50:03.892885",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.643722Z",
     "iopub.execute_input": "2023-08-02T13:25:53.644200Z",
     "iopub.status.idle": "2023-08-02T13:25:53.703178Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.644160Z",
     "shell.execute_reply": "2023-08-02T13:25:53.702030Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.23 Convert Formats\n",
    "\n",
    "We will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n",
    "\n",
    "In this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n",
    "\n",
    "** Developer Documentation: **\n",
    "* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n",
    "* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n",
    "* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ],
   "metadata": {
    "_cell_guid": "f7972928-84b5-47b1-98b6-3e47bc7ddb17",
    "_uuid": "eb0a15c2065a827c3c5431622c04707f3e9d2e52",
    "papermill": {
     "duration": 0.045403,
     "end_time": "2023-05-17T04:50:04.071847",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.026444",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "\n",
    "for data in data_cleaner:\n",
    "\n",
    "    data['HomePlanet_Code'] = label.fit_transform(data['HomePlanet'])\n",
    "    data['CryoSleep_Code'] = label.fit_transform(data['CryoSleep'])\n",
    "    data['Destination_Code'] = label.fit_transform(data['Destination'])\n",
    "    data['VIP_Code'] = label.fit_transform(data['VIP'])\n",
    "    data['Age_group_Code'] = label.fit_transform(data['Age_group'])\n",
    "    data['Cabin_deck_Code'] = label.fit_transform(data['Cabin_deck'])\n",
    "    data['Cabin_side_Code'] = label.fit_transform(data['Cabin_side'])\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.1017,
     "end_time": "2023-05-17T04:50:04.219357",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.117657",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.704662Z",
     "iopub.execute_input": "2023-08-02T13:25:53.705096Z",
     "iopub.status.idle": "2023-08-02T13:25:53.754239Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.705065Z",
     "shell.execute_reply": "2023-08-02T13:25:53.752712Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define y variable aka target/outcome\n",
    "Target = ['Transported']\n",
    "\n",
    "# define x variables for original features aka feature selection\n",
    "\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n",
    "data1_x = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] # Original data\n",
    "# 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'Age_group', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_deck', 'Cabin_number', 'Cabin_side', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'\n",
    "\n",
    "data1_x_calc = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Expenditure', 'No_spending', 'Group', 'Group_size', 'Solo', 'Cabin_number', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'] # coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "# define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Age', 'No_spending', 'Group_size', 'Solo', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ],
   "metadata": {
    "_cell_guid": "95f9a05d-9f6e-46b3-8a3c-95a78f0c0834",
    "_uuid": "7f746d8c6f7d0214253919185fa16f2f6a447a15",
    "papermill": {
     "duration": 0.094534,
     "end_time": "2023-05-17T04:50:04.359877",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.265343",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.766141Z",
     "iopub.execute_input": "2023-08-02T13:25:53.766591Z",
     "iopub.status.idle": "2023-08-02T13:25:53.810847Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.766561Z",
     "shell.execute_reply": "2023-08-02T13:25:53.809556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.637087700Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X Y:  ['Transported', 'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] \n",
      "\n",
      "Bin X Y:  ['Transported', 'Age', 'No_spending', 'Group_size', 'Solo', 'Cabin_region1', 'Cabin_region2', 'Cabin_region3', 'Cabin_region4', 'Cabin_region5', 'Cabin_region6', 'Cabin_region7', 'Family_size', 'HomePlanet_Code', 'CryoSleep_Code', 'Destination_Code', 'VIP_Code', 'Age_group_Code', 'Cabin_deck_Code', 'Cabin_side_Code'] \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 22\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBin X Y: \u001B[39m\u001B[38;5;124m'\u001B[39m, data1_xy_bin, \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#define x and y variables for dummy features original\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m data1_dummy \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(\u001B[43mdata1\u001B[49m[data1_x])\n\u001B[1;32m     23\u001B[0m data1_x_dummy \u001B[38;5;241m=\u001B[39m data1_dummy\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     24\u001B[0m data1_xy_dummy \u001B[38;5;241m=\u001B[39m Target \u001B[38;5;241m+\u001B[39m data1_x_dummy\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data1' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.24 Da-Double Check Cleaned Data\n",
    "\n",
    "Now that we've cleaned our data, let's do a discount da-double check!"
   ],
   "metadata": {
    "_cell_guid": "43ae9ddf-85b6-4abd-89a7-cfb0ff70ebad",
    "_uuid": "9f4b880b2e485e6bc4b6a64113b8f816e2319e85",
    "papermill": {
     "duration": 0.046523,
     "end_time": "2023-05-17T04:50:04.452725",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.406202",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ],
   "metadata": {
    "_cell_guid": "00d10c37-5d8a-4eaf-8fe8-25a0b58e0bfd",
    "_uuid": "e2ef29db7e31dc83c10238ee33ad4ad0ad426183",
    "papermill": {
     "duration": 0.19419,
     "end_time": "2023-05-17T04:50:04.69404",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.49985",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:53.812775Z",
     "iopub.execute_input": "2023-08-02T13:25:53.813555Z",
     "iopub.status.idle": "2023-08-02T13:25:54.002081Z",
     "shell.execute_reply.started": "2023-08-02T13:25:53.813509Z",
     "shell.execute_reply": "2023-08-02T13:25:54.000902Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.699586700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.25 Split Training and Testing Data\n",
    "\n",
    "As mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use [*sklearn's* train_test_split function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). In later sections we will also use [*sklearn's* cross validation functions](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), that splits our dataset into train and test for data modeling comparison."
   ],
   "metadata": {
    "_cell_guid": "e1ffcb68-17a5-43e0-b7e1-4a0b9d33dbdd",
    "_uuid": "9b4b6e1f40e310274447c880760b87ec6a0c7c77",
    "papermill": {
     "duration": 0.049793,
     "end_time": "2023-05-17T04:50:04.791928",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.742135",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ],
   "metadata": {
    "_cell_guid": "15e84f99-e015-4e0a-bd0a-2f856ffad1f6",
    "_uuid": "f9eb9f6e78235a38580ee3125ede17b836edabb3",
    "papermill": {
     "duration": 0.094739,
     "end_time": "2023-05-17T04:50:04.933657",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.838918",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:54.003494Z",
     "iopub.execute_input": "2023-08-02T13:25:54.005260Z",
     "iopub.status.idle": "2023-08-02T13:25:54.051030Z",
     "shell.execute_reply.started": "2023-08-02T13:25:54.005215Z",
     "shell.execute_reply": "2023-08-02T13:25:54.049493Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.699586700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch6\"></a>\n",
    "# Step 4: Perform Exploratory Analysis with Statistics\n",
    "Now that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other."
   ],
   "metadata": {
    "_cell_guid": "8a13c80c-9f6b-405a-bba2-d3fb9b1cf19d",
    "_uuid": "d3d49dc8106082d149dc4058b002355fc809726b",
    "papermill": {
     "duration": 0.047922,
     "end_time": "2023-05-17T04:50:05.029218",
     "exception": false,
     "start_time": "2023-05-17T04:50:04.981296",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Transported Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')      \n"
   ],
   "metadata": {
    "_cell_guid": "f14819d3-e23c-40fa-a281-375e8777a47b",
    "_uuid": "5355b0e45b130a1fd574510141f65e62ebfe2148",
    "papermill": {
     "duration": 0.088385,
     "end_time": "2023-05-17T04:50:05.164475",
     "exception": false,
     "start_time": "2023-05-17T04:50:05.07609",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:54.053285Z",
     "iopub.execute_input": "2023-08-02T13:25:54.053924Z",
     "iopub.status.idle": "2023-08-02T13:25:54.096416Z",
     "shell.execute_reply.started": "2023-08-02T13:25:54.053871Z",
     "shell.execute_reply": "2023-08-02T13:25:54.094470Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.715212800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize': 5 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ],
   "metadata": {
    "_cell_guid": "fbfaf416-4eb7-45fc-8ee2-123852ee6a7d",
    "_uuid": "d599e6d35f40e66f3f2bd162b2d9087d10ed443d",
    "papermill": {
     "duration": 4.910203,
     "end_time": "2023-05-17T04:50:10.122674",
     "exception": false,
     "start_time": "2023-05-17T04:50:05.212471",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:54.098259Z",
     "iopub.execute_input": "2023-08-02T13:25:54.098874Z",
     "iopub.status.idle": "2023-08-02T13:25:57.071319Z",
     "shell.execute_reply.started": "2023-08-02T13:25:54.098839Z",
     "shell.execute_reply": "2023-08-02T13:25:57.070097Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.715212800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Transported', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ],
   "metadata": {
    "papermill": {
     "duration": 805.253745,
     "end_time": "2023-05-17T05:03:35.433222",
     "exception": false,
     "start_time": "2023-05-17T04:50:10.179477",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:25:57.073034Z",
     "iopub.execute_input": "2023-08-02T13:25:57.074083Z",
     "iopub.status.idle": "2023-08-02T13:41:05.966430Z",
     "shell.execute_reply.started": "2023-08-02T13:25:57.074038Z",
     "shell.execute_reply": "2023-08-02T13:41:05.965261Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.730837Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch7\"></a>\n",
    "# Step 5: Model Data\n",
    "Data Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don’t worry, we only need a high-level overview, which we’ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, we’ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, it’s learning from us and not the other way around.\n",
    "\n",
    "Machine Learning (ML), as the name suggest, is teaching the machine how-to think and not what to think. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower, for businesses and professionals alike. This is both good and bad. It’s good because these algorithms are now accessible to more people that can solve more problems in the real-world. It’s bad because a lower barrier to entry means, more people will not know the tools they are using and can come to incorrect conclusions. That’s why I focus on teaching you, not just what to do, but why you’re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that I’ve hammered (no pun intended) my point, I’ll show you what to do and most importantly, WHY you do it.\n",
    "\n",
    "First, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.\n",
    "\n",
    "There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We'll save clustering and dimension reduction for another day, and focus on classification and regression. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the *sklearn* library to begin our analysis. We will use cross validation and scoring metrics, discussed in later sections, to rank and compare our algorithms’ performance.\n",
    "\n",
    "**Machine Learning Selection:**\n",
    "* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)\n",
    "* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)\n",
    "* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "\n",
    "\n",
    "Now that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.\n",
    "\n",
    "**Machine Learning Classification Algorithms:**\n",
    "* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n",
    "* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n",
    "* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n",
    "* [Discriminant Analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)\n",
    "\n",
    "\n",
    "### Data Science 101: How to Choose a Machine Learning Algorithm (MLA)\n",
    "**IMPORTANT:** When it comes to data modeling, the beginner’s question is always, \"what is the best machine learning algorithm?\" To this the beginner must learn, the [No Free Lunch Theorem (NFLT)](http://robertmarks.org/Classes/ENGR5358/Papers/NFL_4_Dummies.pdf) of Machine Learning. In short, NFLT states, there is no super algorithm, that works best in all situations, for all datasets. So the best approach is to try multiple MLAs, tune them, and compare them for your specific scenario. With that being said, some good research has been done to compare algorithms, such as [Caruana & Niculescu-Mizil 2006](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) watch [video lecture here](http://videolectures.net/solomon_caruana_wslmw/) of MLA comparisons, [Ogutu et al. 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3103196/) done by the NIH for genomic selection, [Fernandez-Delgado et al. 2014](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) comparing 179 classifiers from 17 families, [Thoma 2016 sklearn comparison](https://martin-thoma.com/comparing-classifiers/), and there is also a school of thought that says, [more data beats a better algorithm](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html). \n",
    "\n",
    "So with all this information, where is a beginner to start? I recommend starting with [Trees, Bagging, Random Forests, and Boosting](http://jessica2.msri.org/attachments/10778/10778-boost.pdf). They are basically different implementations of a decision tree, which is the easiest concept to learn and understand. They are also easier to tune, discussed in the next section, than something like SVC. Below, I'll give an overview of how-to run and compare several MLAs, but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives."
   ],
   "metadata": {
    "_cell_guid": "079d255c-e55e-482b-be9f-ae6b23015790",
    "_uuid": "1f4f5f75093fdcec864a835d94cd5d9a05b591c5",
    "papermill": {
     "duration": 0.115477,
     "end_time": "2023-05-17T05:03:35.673779",
     "exception": false,
     "start_time": "2023-05-17T05:03:35.558302",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ],
   "metadata": {
    "_cell_guid": "68816e33-19bc-482e-b4e8-062098b18798",
    "_uuid": "979ae81e95cf760f05e94853b932b5101198b4e9",
    "papermill": {
     "duration": 811.229681,
     "end_time": "2023-05-17T05:17:07.02032",
     "exception": false,
     "start_time": "2023-05-17T05:03:35.790639",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-08-02T13:41:05.967849Z",
     "iopub.execute_input": "2023-08-02T13:41:05.968457Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:03.746463600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ],
   "metadata": {
    "_cell_guid": "b9653051-8e79-4ae3-ac31-497bf0a5caed",
    "_uuid": "b7acbf3f7ad6e812a2b5d60794bc3c5ad7d3fd0c",
    "papermill": {
     "duration": 0.877101,
     "end_time": "2023-05-17T05:17:08.013222",
     "exception": false,
     "start_time": "2023-05-17T05:17:07.136121",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:04.174228700Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241m.\u001B[39mbarplot(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMLA Test Accuracy Mean\u001B[39m\u001B[38;5;124m'\u001B[39m, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMLA Name\u001B[39m\u001B[38;5;124m'\u001B[39m, data \u001B[38;5;241m=\u001B[39m MLA_compare, color \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\u001B[39;00m\n\u001B[1;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMachine Learning Algorithm Accuracy Score \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch8\"></a>\n",
    "## 5.1 Evaluate Model Performance\n",
    "Let's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger transportwith ~80% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind when improving your model.\n",
    "\n",
    "### Data Science 101: Determine a Baseline Accuracy ###\n",
    "Before we decide how-to make our model better, let's determine if our model is even worth keeping. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers transported or not. So, think of it like a coin flip problem. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing correct. So, let's set 50% as the worst model performance; because anything lower than that, then why do I need you when I can just flip a coin?\n",
    "\n",
    "Okay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 4378/8693 or 50.4% of people transported. Therefore, if we just predict the most frequent occurrence, that 100% of people transported, then we would be right 50.3% of the time. So, let's set 51% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.\n",
    "\n",
    "### Data Science 101: How-to Create Your Own Model ###\n",
    "Our accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that segment your target response, placing the transported/1 and not transported/0 into homogeneous subgroups. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n",
    "\n",
    "Remember, the name of the game is to create subgroups using a decision tree model to get transported/1 in one bucket and not transported/0 in another bucket. Our rule of thumb will be the majority rules. Got it? Let's go!\n",
    "\n",
    "***Question 1*** Adolescents aged 0-18 were transported more often than not.\n",
    "\n",
    "***Question 2*** People who were transported tended to spend less.\n",
    "\n",
    "***Question 3*** People from the planet Earth are less fortunate in transportation, but people from the planet Europe, on the contrary, are more fortunate.\n",
    "\n",
    "***Question 4*** The people in the cryopod most likely escaped.\n",
    "\n",
    "***Question 5*** The location of the individual cabins also had an impact on the transportation of people.\n",
    "\n",
    "You did it, with very little information, we get to 70% accuracy. On a worst, bad, good, better, and best scale, we'll set 70% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model? \n",
    "\n",
    "Before we do, let's code what we just wrote above. Please note, this is a manual process created by \"hand.\" You won't have to do this, but it's important to understand it before you start working with MLA. Think of MLA like a TI-89 calculator on a Calculus Exam. It's very powerful and helps you with a lot of the grunt work. But if you don't know what you're doing on the exam, a calculator, even a TI-89, is not going to help you pass. So, study the next section wisely.\n",
    "\n",
    "Reference: [Cross-Validation and Decision Tree Tutorial](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf)"
   ],
   "metadata": {
    "_cell_guid": "c90ba48e-3c44-47c8-9bd2-d46187efb9d8",
    "_uuid": "9e3ac7492a21e7f60b0656008f7873cab882f196",
    "papermill": {
     "duration": 0.200036,
     "end_time": "2023-05-17T05:17:08.343602",
     "exception": false,
     "start_time": "2023-05-17T05:17:08.143566",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.at[index, 'Random_Predict'] = 1 # predict survived/1\n",
    "    else: \n",
    "        data1.at[index, 'Random_Predict'] = 0 # predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Transported'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1[Target], data1['Random_Predict'])*100))\n"
   ],
   "metadata": {
    "_cell_guid": "7927b0e9-d000-4a67-a090-1fa19579e3ee",
    "_uuid": "fba1acea71030c07c882deff78b917c19c8b7585",
    "papermill": {
     "duration": 0.772393,
     "end_time": "2023-05-17T05:17:09.23505",
     "exception": false,
     "start_time": "2023-05-17T05:17:08.462657",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:04.189862300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data1.info()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.152777,
     "end_time": "2023-05-17T05:17:09.508393",
     "exception": false,
     "start_time": "2023-05-17T05:17:09.355616",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T08:47:04.189862300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "pivot_age = data1.groupby(['Age_group'])['Transported'].mean()\n",
    "print('Survival Decision Tree age Node: \\n',pivot_age)\n",
    "\n",
    "pivot_vip = data1.groupby(['VIP_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree vip Node: \\n',pivot_vip)\n",
    "\n",
    "pivot_HomePlanet = data1.groupby(['HomePlanet_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree HomePlanet Node: \\n',pivot_HomePlanet)\n",
    "\n",
    "pivot_CryoSleep = data1.groupby(['CryoSleep_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree CryoSleep Node: \\n',pivot_CryoSleep)\n",
    "\n",
    "pivot_Cabin_deck_Code = data1.groupby(['Cabin_deck_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_deck_Code Node: \\n',pivot_Cabin_deck_Code)\n",
    "\n",
    "pivot_Cabin_side_Code = data1.groupby(['Cabin_side_Code'])['Transported'].mean()\n",
    "print('\\n Survival Decision Tree Cabin_side_Code Node: \\n',pivot_Cabin_side_Code)\n",
    "\n",
    "# pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Transported'].mean()\n",
    "# print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)"
   ],
   "metadata": {
    "_cell_guid": "cde02521-1ec1-4293-b8de-064c49118d91",
    "_uuid": "c371166144bb9cd7925c3b8a3a00baf90bcebfb9",
    "papermill": {
     "duration": 0.149968,
     "end_time": "2023-05-17T05:17:09.776572",
     "exception": false,
     "start_time": "2023-05-17T05:17:09.626604",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #Question 1: Age_group (55-69%)\n",
    "        if (df.loc[index, 'Age_group'] == 'Age_0-12') or (df.loc[index, 'Age_group'] == 'Age_13-17'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "                \n",
    "        #Question 2: HomePlanet_Code (66-67%)\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'HomePlanet_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1 \n",
    "        \n",
    "        #Question 3: VIP_Code (71%)\n",
    "        if (df.loc[index, 'VIP_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        \n",
    "        #Question 4: CryoSleep_Code (68-81%)\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 0):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'CryoSleep_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1  \n",
    "                \n",
    "        #Question 5: Cabin_deck_Code_Code (73-80%)\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 7):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 1):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "        if (df.loc[index, 'Cabin_deck_Code'] == 4):\n",
    "                  Model.loc[index, 'Predict'] = 0                \n",
    "        \n",
    "        #Question 6: Cabin_side_Code (72%)\n",
    "        if (df.loc[index, 'Cabin_side_Code'] == 2):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "                \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Transported'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Transported'], Tree_Predict))\n",
    "\n"
   ],
   "metadata": {
    "_cell_guid": "330582d5-8034-4c4a-93f0-60067939c5c0",
    "_uuid": "82a3b4a54c2cb510f790ab2f7e3c52c46b63206b",
    "papermill": {
     "duration": 8.469389,
     "end_time": "2023-05-17T05:17:18.368508",
     "exception": false,
     "start_time": "2023-05-17T05:17:09.899119",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Transported'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['NotTransported', 'Transported']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "\n"
   ],
   "metadata": {
    "_cell_guid": "12c7bfa9-6876-4df7-adf9-147eac1bd43c",
    "_uuid": "22413208c3179422ff603c059c695d78b4dd0db1",
    "papermill": {
     "duration": 1.237802,
     "end_time": "2023-05-17T05:17:19.726504",
     "exception": false,
     "start_time": "2023-05-17T05:17:18.488702",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.11 Model Performance with Cross-Validation (CV)\n",
    "In step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n",
    "\n",
    "Remember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%, but then when you go to take the exam, you fail because you never truly learned anything. The same is true with machine learning.\n",
    "\n",
    "CV is basically a shortcut to split and score our model multiple times, so we can get an idea of how well it will perform on unseen data. It’s a little more expensive in computer processing, but it's important so we don't gain false confidence. This is helpful in a Kaggle Competition or any use case where consistency matters and surprises should be avoided.\n",
    " \n",
    "In addition to CV, we used a customized [sklearn train test splitter](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), to allow a little more randomness in our test scoring. Below is an image of the default CV split.\n"
   ],
   "metadata": {
    "_cell_guid": "ec85d77e-0be8-457e-b876-ccf3545d35e5",
    "_uuid": "9e6bf62b0e2a7f8ce2cccd68310338dfc5d2c156",
    "papermill": {
     "duration": 0.121784,
     "end_time": "2023-05-17T05:17:19.979085",
     "exception": false,
     "start_time": "2023-05-17T05:17:19.857301",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch9\"></a>\n",
    "# 5.12 Tune Model with Hyper-Parameters\n",
    "When we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n",
    "\n",
    "However, in order to tune a model, we need to actually understand it. That's why I took the time in the previous sections to show you how predictions work. Now let's learn a little bit more about our DT algorithm.\n",
    "\n",
    "Credit: [sklearn](http://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    ">**Some advantages of decision trees are:**\n",
    "* Simple to understand and to interpret. Trees can be visualized.\n",
    "* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "* Able to handle multi-output problems.\n",
    "* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "> **The disadvantages of decision trees include:**\n",
    "* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "\n",
    "\n",
    "Below are available hyper-parameters and [defintions](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier):\n",
    "> class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "\n",
    "We will tune our model using [ParameterGrid](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid), [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), and customized [sklearn scoring](http://scikit-learn.org/stable/modules/model_evaluation.html); [click here to learn more about ROC_AUC scores](http://www.dataschool.io/roc-curves-and-auc-explained/). We will then visualize our tree with [graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). [Click here to learn more about ROC_AUC scores](http://www.dataschool.io/roc-curves-and-auc-explained/).\n"
   ],
   "metadata": {
    "_cell_guid": "1ae78385-67ec-431e-98ad-1c34ab10499b",
    "_uuid": "68978ae17037762ad30405bb51ec67326fecfdec",
    "papermill": {
     "duration": 0.122002,
     "end_time": "2023-05-17T05:17:20.222415",
     "exception": false,
     "start_time": "2023-05-17T05:17:20.100413",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#base model\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "dtree.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "tune_model.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#duplicates gridsearchcv\n",
    "#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n",
    "#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n",
    "#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n",
    "#print('-'*10)\n"
   ],
   "metadata": {
    "_cell_guid": "a614a6a0-1861-4d87-aaab-cc8f9b54bd82",
    "_uuid": "4cfe6cb996dd39e1145effffa51a6953c396278d",
    "papermill": {
     "duration": 4.222378,
     "end_time": "2023-05-17T05:17:24.566684",
     "exception": false,
     "start_time": "2023-05-17T05:17:20.344306",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch10\"></a>\n",
    "## 5.13 Tune Model with Feature Selection\n",
    "As stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV)."
   ],
   "metadata": {
    "_cell_guid": "3e84f865-d316-4268-a422-c0062e4069c2",
    "_uuid": "cf8c9846dfcbd7277756f1a573d55642d3f2f2be",
    "papermill": {
     "duration": 0.129621,
     "end_time": "2023-05-17T05:17:24.818864",
     "exception": false,
     "start_time": "2023-05-17T05:17:24.689243",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True )\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ],
   "metadata": {
    "_cell_guid": "df54f00c-e5a2-4994-aab8-f5b19961eafb",
    "_uuid": "d35bf5c10d2dc9c582e5fea4956942182c6fd206",
    "papermill": {
     "duration": 5.928682,
     "end_time": "2023-05-17T05:17:30.869785",
     "exception": false,
     "start_time": "2023-05-17T05:17:24.941103",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ],
   "metadata": {
    "_cell_guid": "a5737627-e5ea-480f-883b-eea1caaa0e46",
    "_uuid": "7337e6c91797354d8fd887a538bf916a80619069",
    "papermill": {
     "duration": 14.440389,
     "end_time": "2023-05-17T05:17:45.431399",
     "exception": false,
     "start_time": "2023-05-17T05:17:30.99101",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch11\"></a>\n",
    "# Step 6: Validate and Implement\n",
    "The next step is to prepare for submission using the validation data. "
   ],
   "metadata": {
    "_cell_guid": "5f9bd02f-93ae-4cfa-992c-38b24c322011",
    "_uuid": "6e2d288d4e477837f513e37461754256e1eef44a",
    "papermill": {
     "duration": 0.349702,
     "end_time": "2023-05-17T05:17:46.132025",
     "exception": false,
     "start_time": "2023-05-17T05:17:45.782323",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)"
   ],
   "metadata": {
    "_cell_guid": "336c5075-9077-479d-9d95-38b6c9b7b0c8",
    "_uuid": "3648c38f48305bacd85ee8ee3048404631a03ad5",
    "papermill": {
     "duration": 3.905941,
     "end_time": "2023-05-17T05:17:50.360932",
     "exception": false,
     "start_time": "2023-05-17T05:17:46.454991",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#why choose one model, when you can pick them all with voting classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n",
    "\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True )\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ],
   "metadata": {
    "_cell_guid": "82d84146-6a2a-4b61-be81-922b3f35542f",
    "_uuid": "80c3637e384b48e64f00462eb8931196457716d2",
    "papermill": {
     "duration": 1366.336384,
     "end_time": "2023-05-17T05:40:37.054917",
     "exception": false,
     "start_time": "2023-05-17T05:17:50.718533",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#IMPORTANT: THIS SECTION IS UNDER CONSTRUCTION!!!!\n",
    "#UPDATE: This section was scrapped for the next section; as it's more computational friendly.\n",
    "\n",
    "#WARNING: Running is very computational intensive and time expensive\n",
    "#code is written for experimental/developmental purposes and not production ready\n",
    "\n",
    "\n",
    "#tune each estimator before creating a super model\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [50,100,300]\n",
    "grid_ratio = [.1,.25,.5,.75,1.0]\n",
    "grid_learn = [.01,.03,.05,.1,.25]\n",
    "grid_max_depth = [2,4,6,None]\n",
    "grid_min_samples = [5,10,.03,.05,.10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "vote_param = [{\n",
    "#            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'ada__n_estimators': grid_n_estimator,\n",
    "            'ada__learning_rate': grid_ratio,\n",
    "            'ada__algorithm': ['SAMME', 'SAMME.R'],\n",
    "            'ada__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'bc__n_estimators': grid_n_estimator,\n",
    "            'bc__max_samples': grid_ratio,\n",
    "            'bc__oob_score': grid_bool, \n",
    "            'bc__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'etc__n_estimators': grid_n_estimator,\n",
    "            'etc__criterion': grid_criterion,\n",
    "            'etc__max_depth': grid_max_depth,\n",
    "            'etc__random_state': grid_seed,\n",
    "\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            'gbc__loss': ['deviance', 'exponential'],\n",
    "            'gbc__learning_rate': grid_ratio,\n",
    "            'gbc__n_estimators': grid_n_estimator,\n",
    "            'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "            'gbc__max_depth': grid_max_depth,\n",
    "            'gbc__min_samples_split': grid_min_samples,\n",
    "            'gbc__min_samples_leaf': grid_min_samples,      \n",
    "            'gbc__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'rfc__n_estimators': grid_n_estimator,\n",
    "            'rfc__criterion': grid_criterion,\n",
    "            'rfc__max_depth': grid_max_depth,\n",
    "            'rfc__min_samples_split': grid_min_samples,\n",
    "            'rfc__min_samples_leaf': grid_min_samples,   \n",
    "            'rfc__bootstrap': grid_bool,\n",
    "            'rfc__oob_score': grid_bool, \n",
    "            'rfc__random_state': grid_seed,\n",
    "        \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'lr__fit_intercept': grid_bool,\n",
    "            'lr__penalty': ['l1','l2'],\n",
    "            'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'lr__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'bnb__alpha': grid_ratio,\n",
    "            'bnb__prior': grid_bool,\n",
    "            'bnb__random_state': grid_seed,\n",
    "    \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'knn__n_neighbors': [1,2,3,4,5,6,7],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "            'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'knn__random_state': grid_seed,\n",
    "            \n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'svc__C': grid_max_depth,\n",
    "            'svc__gamma': grid_ratio,\n",
    "            'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "            'svc__probability': [True],\n",
    "            'svc__random_state': grid_seed,\n",
    "    \n",
    "    \n",
    "            #http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'xgb__learning_rate': grid_ratio,\n",
    "            'xgb__max_depth': [2,4,6,8,10],\n",
    "            'xgb__tree_method': ['exact', 'approx', 'hist'],\n",
    "            'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n",
    "            'xgb__seed': grid_seed  \n",
    "\n",
    "        }]\n",
    "\n",
    "#Soft Vote with tuned models\n",
    "#grid_soft = model_selection.GridSearchCV(estimator = vote_soft, param_grid = vote_param, cv = 2, scoring = 'roc_auc')\n",
    "#grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#print(grid_soft.cv_results_.keys())\n",
    "#print(grid_soft.cv_results_['params'])\n",
    "#print('Soft Vote Tuned Parameters: ', grid_soft.best_params_)\n",
    "#print(grid_soft.cv_results_['mean_train_score'])\n",
    "#print(\"Soft Vote Tuned Training w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(grid_soft.cv_results_['mean_test_score'])\n",
    "#print(\"Soft Vote Tuned Test w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "#print(\"Soft Vote Tuned Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "#print('-'*10)\n",
    "\n",
    "\n",
    "#credit: https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/\n",
    "#cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "#for r, _ in enumerate(grid_soft.cv_results_['mean_test_score']):\n",
    "#    print(\"%0.3f +/- %0.2f %r\"\n",
    "#          % (grid_soft.cv_results_[cv_keys[0]][r],\n",
    "#             grid_soft.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "#             grid_soft.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "#print('-'*10)\n"
   ],
   "metadata": {
    "_cell_guid": "c62f344d-24c3-4c3d-8a7a-7d1013390091",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "20d362a06ca426b4bc8b2413209885f9f7853a3f",
    "papermill": {
     "duration": 0.354184,
     "end_time": "2023-05-17T05:40:37.738145",
     "exception": false,
     "start_time": "2023-05-17T05:40:37.383961",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#WARNING: Running is very computational intensive and time expensive.\n",
    "#Code is written for experimental/developmental purposes and not production ready!\n",
    "\n",
    "#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=’deviance’\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = ‘uniform’\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[data1_x_bin], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ],
   "metadata": {
    "_cell_guid": "7b605629-06f5-4bdb-b7da-c0f842ef678f",
    "_uuid": "77e83a4e5a31b9276767f550cca0de1bc53f5f70",
    "papermill": {
     "duration": 10001.951068,
     "end_time": "2023-05-17T08:27:20.01545",
     "exception": false,
     "start_time": "2023-05-17T05:40:38.064382",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Hard Vote or majority rules w/Tuned Hyperparameters\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Soft Vote or weighted probabilities w/Tuned Hyperparameters\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "grid_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ],
   "metadata": {
    "_cell_guid": "309fed2c-44d2-481c-807a-bd062f5a329c",
    "_uuid": "b09f7cbe41527b36e4d3754febda6c1b395bd7e9",
    "papermill": {
     "duration": 1538.5815,
     "end_time": "2023-05-17T08:52:58.922124",
     "exception": false,
     "start_time": "2023-05-17T08:27:20.340624",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "#handmade decision tree - submission score = 0.77990\n",
    "# data_val['Transported'] = mytree(data_val).astype(int)  # 0 V7\n",
    "data_val['Transported'] = mytree(data_val)\n",
    "\n",
    "#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_dt = tree.DecisionTreeClassifier()\n",
    "#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_dt.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n",
    "#submit_bc = ensemble.BaggingClassifier()\n",
    "#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_bc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n",
    "#submit_etc = ensemble.ExtraTreesClassifier()\n",
    "#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_etc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n",
    "#submit_rfc = ensemble.RandomForestClassifier()\n",
    "#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n",
    "#submit_abc = ensemble.AdaBoostClassifier()\n",
    "#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_abc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n",
    "#submit_gbc = ensemble.GradientBoostingClassifier()\n",
    "#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n",
    "#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n",
    "\n",
    "#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n",
    "#submit_xgb = XGBClassifier()\n",
    "#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n",
    "#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n",
    "#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n",
    "#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n",
    "\n",
    "\n",
    "#hard voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.74655 V4\n",
    "# data_val['Transported'] = vote_hard.predict(data_val[data1_x_bin])  # 0.74655 V4\n",
    "# data_val['Transported'] = grid_hard.predict(data_val[data1_x_bin])  # 0.70189 V4\n",
    "\n",
    "\n",
    "#soft voting classifier w/full dataset modeling submission score: defaults=-, tuned = 0.75005 V6\n",
    "# data_val['Transported'] = vote_soft.predict(data_val[data1_x_bin])  # 0.75005 V6\n",
    "# data_val['Transported'] = grid_soft.predict(data_val[data1_x_bin])  # 0.74982 V5\n",
    "\n",
    "\n",
    "#submit file\n",
    "submit = data_val[['PassengerId','Transported']]\n",
    "submit.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Transported'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "\n",
    "# The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 147.10 seconds.\n",
    "# The best parameter for BaggingClassifier is {'max_samples': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 259.10 seconds.\n",
    "# The best parameter for ExtraTreesClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'random_state': 0} with a runtime of 268.24 seconds.\n",
    "# The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'random_state': 0} with a runtime of 484.47 seconds.\n",
    "# The best parameter for RandomForestClassifier is {'criterion': 'gini', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 376.14 seconds.\n",
    "# The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 1475.33 seconds.\n",
    "# The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 433.93 seconds.\n",
    "# The best parameter for BernoulliNB is {'alpha': 0.5} with a runtime of 1.08 seconds.\n",
    "# The best parameter for GaussianNB is {} with a runtime of 0.25 seconds.\n",
    "# The best parameter for KNeighborsClassifier is {'algorithm': 'ball_tree', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 46.03 seconds.\n",
    "# The best parameter for SVC is {'C': 1, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 5311.01 seconds.\n",
    "# The best parameter for XGBClassifier is {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'seed': 0} with a runtime of 597.68 seconds.\n",
    "# Total optimization time was 156.67 minutes."
   ],
   "metadata": {
    "_cell_guid": "057f369f-a3c6-4718-94f4-db8cb6d59777",
    "_uuid": "29ad86c5413910763316df7fda7753c4b0d0a97d",
    "papermill": {
     "duration": 4.941159,
     "end_time": "2023-05-17T08:53:04.19632",
     "exception": false,
     "start_time": "2023-05-17T08:52:59.255161",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ch12\"></a>\n",
    "# Step 7: Optimize and Strategize\n",
    "## Conclusion\n",
    "Iteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\n",
    "\n",
    "While no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \n",
    "1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\n",
    "2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\n",
    "3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \n",
    "\n",
    "With that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\n",
    "\n",
    "![](https://i.postimg.cc/P5sJ7FSd/kitaj-postroit-gigantskij-kosmicheskij-korabl.jpg)"
   ],
   "metadata": {
    "_cell_guid": "a6d34edc-1265-41e8-b163-0b817cd3b6a5",
    "_uuid": "c476df95925d67809975caca4ea4dcb4b2a0b277",
    "papermill": {
     "duration": 0.326583,
     "end_time": "2023-05-17T08:53:04.856681",
     "exception": false,
     "start_time": "2023-05-17T08:53:04.530098",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "_cell_guid": "51f197c2-76d7-4503-ade5-f9f35323c4c5",
    "_uuid": "2e95a403934cf5c0ab1ff9148b62ef14590e84c9",
    "papermill": {
     "duration": 0.374496,
     "end_time": "2023-05-17T08:53:05.580132",
     "exception": false,
     "start_time": "2023-05-17T08:53:05.205636",
     "status": "completed"
    },
    "tags": []
   }
  }
 ]
}
